{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/results?search_query=Restaurant&page=1\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=2\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=3\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=4\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=5\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=6\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=7\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=8\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=9\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=10\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=11\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=12\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=13\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=14\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=15\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=16\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=17\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=18\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=19\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=20\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=21\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=22\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=23\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=24\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=25\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=26\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=27\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=28\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=29\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=30\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=31\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=32\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=33\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=34\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=35\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=36\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=37\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=38\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=39\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=40\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=41\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=42\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=43\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=44\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=45\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=46\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=47\n",
      "https://www.youtube.com/results?search_query=Restaurant&page=48\n"
     ]
    }
   ],
   "source": [
    "# Scrapping of most viewed videos on YOutube\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "\n",
    "surl = \"https://www.youtube.com/results?search_query=\"\n",
    "\n",
    "topics = [ \"Restaurant\", \"food\", \"Restaurant Style Biryani\",\"Restaurant Style Dragon Chicken\",\n",
    "           \"kitchen\",\"foodie\", \"eating\", \"street food\", \"restraunts\"]\n",
    "\n",
    "page = \"&page=\"\n",
    "out = open(\"output.csv\", \"a+\")\n",
    "\n",
    "for topic in topics:\n",
    "    page_count = 1\n",
    "    total_pages = 50\n",
    "    count = 1\n",
    "\n",
    "    while page_count < total_pages:\n",
    "        url = surl + topic + page + str(page_count)\n",
    "        print(url)\n",
    "        headers = {'Accept-Language': 'en-US,en;q=0.8'}\n",
    "        r = requests.get(url,headers=headers)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        videos = soup.find_all(\"div\", class_=\"yt-lockup-video\")\n",
    "        if videos is not None:\n",
    "            for vid in videos:\n",
    "                title = vid.find(\"a\", \"yt-uix-tile-link\").get('title')\n",
    "                link = vid.find(\"a\", \"yt-uix-tile-link\").get('href')\n",
    "                title = unidecode(title)\n",
    "                div = vid.find(\"div\", \"yt-lockup-byline\")\n",
    "                channel = div.find(\"a\", \"yt-uix-sessionlink spf-link\").text\n",
    "                channel = unidecode(channel)\n",
    "                channel_link = div.find(\"a\", \"yt-uix-sessionlink spf-link\").get('href')\n",
    "                info = [\"\", \"\"]\n",
    "                ul = vid.find(\"ul\", \"yt-lockup-meta-info\")\n",
    "                tm = ul.find_all(\"li\")\n",
    "                i = 0\n",
    "                for item in tm:\n",
    "                    if i < 2:\n",
    "                        info[i] = item.text\n",
    "                        info[i] = unidecode(info[i])\n",
    "                        i += 1\n",
    "\n",
    "                # if vid.find(\"div\", \"yt-lockup-description yt-ui-ellipsis yt-ui-ellipsis-2\") is not None:\n",
    "                #   des = vid.find(\"div\", \"yt-lockup-description yt-ui-ellipsis yt-ui-ellipsis-2\").text\n",
    "                # else:\n",
    "                #   des = \"\"\n",
    "\n",
    "                url_vid = \"https://www.youtube.com/\" + link\n",
    "                req_vid = requests.get(url_vid,headers=headers)\n",
    "                soup_vid = BeautifulSoup(req_vid.content, \"html.parser\")\n",
    "\n",
    "                if soup_vid.find(\"button\",\n",
    "                                 \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-like-button like-button-renderer-like-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\") is not None:\n",
    "                    likes = soup_vid.find(\"button\",\n",
    "                                          \"yt-uix-button yt-uix-button-size-default yt-uix-button-opacity yt-uix-button-has-icon no-icon-markup like-button-renderer-like-button like-button-renderer-like-button-unclicked yt-uix-clickcard-target yt-uix-tooltip\")\n",
    "                    like = likes.find(\"span\").text\n",
    "                    like = unidecode(like)\n",
    "                else:\n",
    "                    like = \"\"\n",
    "\n",
    "                disd = soup_vid.find(\"button\",{\"title\":\"I dislike this\"})\n",
    "                if soup_vid.find(\"button\",{\"title\":\"I dislike this\"}) is not None:\n",
    "                    if disd.find(\"span\") is not None:\n",
    "                        dis = disd.find(\"span\").text\n",
    "                else:\n",
    "                    dis = \"\"\n",
    "\n",
    "                des=\"\"\n",
    "                if soup_vid.find(\"div\",{\"id\":\"watch-description-text\"}) is not None:\n",
    "                    div = soup_vid.find_all(\"div\",{\"id\":\"watch-description-text\"})\n",
    "                    for desc in div:\n",
    "                        des = des + desc.text\n",
    "                        des = unidecode(des)\n",
    "\n",
    "                if soup_vid.find(\"meta\", property=\"og:keywords\") is not None:\n",
    "                    keywords = soup_vid.find(\"meta\", property=\"og:keywords\", content=True).get('content')\n",
    "                    keywords = unidecode(keywords)\n",
    "                else:\n",
    "                    keywords = \"\"\n",
    "\n",
    "                if soup_vid.find(\"span\",\n",
    "                                 \"yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count\") is not None:\n",
    "                    subs = soup_vid.find(\"span\",\n",
    "                                         \"yt-subscription-button-subscriber-count-branded-horizontal yt-subscriber-count\").text\n",
    "                    subs = unidecode(subs)\n",
    "                else:\n",
    "                    subs = \"\"\n",
    "                title = title.replace(',', '')\n",
    "                des = des.replace(',', '')\n",
    "                subs = subs.replace(',', '')\n",
    "                info[0] = info[0].replace(',', '')\n",
    "                info[1] = info[1].replace(',', '')\n",
    "                like = like.replace(',', '')\n",
    "                dis = dis.replace(',',\"\")\n",
    "                channel = channel.replace(',', '')\n",
    "                print(str(count) + ' ' + title + ',' + link + ',' + keywords + ',' + channel + ',' + channel_link + ',' + subs + ',' +\n",
    "                      info[0] + ',' + info[1] + ',' + like + ',' + dis + ',' + des)\n",
    "                out.write(title + ',' + channel + ',' + subs + ',' + info[0] + ',' + info[1] + ',' + like + ',' + dis + ',' + des+'\\n')\n",
    "                count += 1\n",
    "                print('\\n')\n",
    "            page_count += 1\n",
    "        else:\n",
    "            page_count = total_pages\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27079ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#India'S International Fixture\n",
    "\n",
    "{\n",
    "  \"fixtures\": {\n",
    "    \"content\": [\n",
    "      {},\n",
    "      {},\n",
    "      {}\n",
    "    ],\n",
    "    \"pageInfo\": {}\n",
    "  },\n",
    "  \"ranking\": [\n",
    "    {\n",
    "      \"t20i\": {}\n",
    "    },\n",
    "    {\n",
    "      \"odi\": {}\n",
    "    },\n",
    "    {\n",
    "      \"test\": {}\n",
    "    }\n",
    "  ],\n",
    "  \"results\": {\n",
    "    \"content\": [\n",
    "      {},\n",
    "      {},\n",
    "      {}\n",
    "    ],\n",
    "    \"pageInfo\": {}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"content\": [\n",
    "    {},\n",
    "    {}\n",
    "  ],\n",
    "  \"pageInfo\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfcae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "State-wise GDP of India from statisticstime.com.\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('NITI_2020_DATA.csv', index_col=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d561c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.values\n",
    "filename = 'gdp_model.sav'\n",
    "scalerfile = 'gdp_scale.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "loaded_scaler = pickle.load(open(scalerfile, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = []\n",
    "for state in data.iterrows():\n",
    "    scaled = loaded_scaler.transform(state[1].values.reshape(1, -1))\n",
    "    predicted = loaded_model.predict(scaled)\n",
    "    predicted_data.append(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5412fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_frame = pd.DataFrame({'State':data.index.values, 'GDP_PREDICTED_2020':predicted_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.8f' %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selenium exception from guru99.com\n",
    "\n",
    "import org.openqa.selenium.By;\t\t\n",
    "import org.openqa.selenium.WebDriver;\t\t\n",
    "import org.openqa.selenium.chrome.ChromeDriver;\t\t\n",
    "import org.openqa.selenium.*;\t\t\n",
    "\n",
    "public class ToolTip {\t\t\t\t\n",
    "    public static void main(String[] args) {\t\t\t\t\t\t\t\t\t\n",
    "     \n",
    "        String baseUrl = \"http://demo.guru99.com/test/social-icon.html\";\t\t\t\t\t\n",
    "        System.setProperty(\"webdriver.chrome.driver\",\"G:\\\\chromedriver.exe\");\t\t\t\t\t\n",
    "        WebDriver driver = new ChromeDriver();\t\t\t\t\t\n",
    "        driver.get(baseUrl);\t\t\t\t\t\n",
    "        String expectedTooltip = \"Github\";\t\n",
    "        \n",
    "        // Find the Github icon at the top right of the header\t\t\n",
    "        WebElement github = driver.findElement(By.xpath(\".//*[@class='soc-ico show-round']/a[4]\"));\t\n",
    "        \n",
    "        //get the value of the \"title\" attribute of the github icon\t\t\n",
    "        String actualTooltip = github.getAttribute(\"title\");\t\n",
    "        \n",
    "        //Assert the tooltip's value is as expected \t\t\n",
    "        System.out.println(\"Actual Title of Tool Tip\"+actualTooltip);\t\t\t\t\t\t\t\n",
    "        if(actualTooltip.equals(expectedTooltip)) {\t\t\t\t\t\t\t\n",
    "            System.out.println(\"Test Case Passed\");\t\t\t\t\t\n",
    "        }\t\t\n",
    "        driver.close();\t\t\t\n",
    "   }\t\t\n",
    "}\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 100 songs on billiboard.com\n",
    "\n",
    "\n",
    "from urllib.request import urlopen as uRequest\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "url = 'https://www.billboard.com/charts/hot-100'\n",
    "\n",
    "# Opening up connection, grabbing the page\n",
    "uClient = uRequest(url)\n",
    "page_html = uClient.read() # Offloads content into a variable\n",
    "uClient.close() # Close the client\n",
    "\n",
    "# HTML parsing\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "# Grabs all information related to the top 100 songs\n",
    "containers = page_soup.select('article[class*=chart]') # *= means contains\n",
    "\n",
    "filename = 'billboard_hot_100.csv'\n",
    "f = open(filename, 'w') # w = write\n",
    "\n",
    "headers = 'Song, Artist, Last Week, Peak Position, Weeks on Chart\\n'\n",
    "\n",
    "f.write(headers)\n",
    "\n",
    "print('Welcome to Jaimes Subroto\\'s Billboard Hot 100 Python Web Scraper!')\n",
    "\n",
    "# Asks the user if he/she wants the data to be printed to the console\n",
    "while True:\n",
    "    print_data = input('Would you like the data to be printed to the console? ')\n",
    "    if print_data.lower() == 'yes' or print_data.lower() == 'y':\n",
    "        print_data = True\n",
    "        print('\\nPrinting this week\\'s HOTTEST 100 songs...')\n",
    "        break\n",
    "    elif print_data.lower() == 'no' or print_data.lower() == 'n':\n",
    "        print_data = False\n",
    "        print('\\nScraping this week\\'s HOTTEST 100 songs...')\n",
    "        break\n",
    "    else:\n",
    "        print('Sorry, I didn\\'t get that.')\n",
    "\n",
    "chart_position = 1\n",
    "\n",
    "# Loops through each container\n",
    "for container in containers:\n",
    "\n",
    "    # Container storing the song name and artist name\n",
    "    song_container = container.find('div', {'class': 'chart-row__title'})\n",
    "\n",
    "    # Grabs the song name\n",
    "    song = song_container.h2.text\n",
    "    \n",
    "    # Grabs the artist name\n",
    "    try:\n",
    "        artist = song_container.a.text.strip()\n",
    "    except AttributeError:\n",
    "        artist = song_container.span.text.strip()\n",
    "\n",
    "    # Grabs the song's position last week\n",
    "    last_week_container = container.find('div', {'class': 'chart-row__last-week'})\n",
    "    last_week = last_week_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Grabs the song's peak position\n",
    "    peak_position_container = container.find('div', {'class': 'chart-row__top-spot'})\n",
    "    peak_position = peak_position_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Grabs the song's duration in the hot 100 (in weeks)\n",
    "    weeks_on_chart_container = container.find('div', {'class': 'chart-row__weeks-on-chart'})\n",
    "    weeks_on_chart = weeks_on_chart_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Prints the chart position, song name, artist name, and related stats\n",
    "    if print_data:\n",
    "        print('\\nPosition: #{}'.format(chart_position))\n",
    "        print('Song: {}'.format(song))\n",
    "        print('Artist: {}'.format(artist))\n",
    "        print('Last Week: {}'.format(last_week))\n",
    "        print('Peak Position: {}'.format(peak_position))\n",
    "        print('Weeks on Chart: {}'.format(weeks_on_chart))\n",
    "\n",
    "    chart_position += 1\n",
    "\n",
    "    f.write('\\\"' + song + '\\\",\\\"' + artist.replace('Featuring', 'Feat.') + '\\\",' + last_week + ',' + peak_position + ',' + weeks_on_chart + '\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('\\nWeb scraped data saved to {}'.format(filename))\n",
    "print('Thank you for using Jaimes Subroto\\'s Web Scraping app for Billboard HOT 100!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science recruiters from naukri.com.\n",
    "\n",
    "# load library\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# chromedriver url link\n",
    "driver=webdriver.Chrome('C:\\Program Files (x86)\\Google\\chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aed3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "jobs={\"roles\":[],\n",
    "     \"companies\":[],\n",
    "     \"locations\":[],\n",
    "     \"experience\":[],\n",
    "     \"skills\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a238beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    driver.get(\"https://www.naukri.com/data-scientist-jobs-{}\".format(i))\n",
    "    time.sleep(3)\n",
    "    lst=driver.find_elements_by_css_selector(\".jobTuple.bgWhite.br4.mb-8\")\n",
    "    \n",
    "    # scrape the data from website\n",
    "    for job in lst:\n",
    "        driver.implicitly_wait(10)\n",
    "        role=job.find_element_by_css_selector(\"a.title.fw500.ellipsis\").text\n",
    "        company=job.find_element_by_css_selector(\"a.subTitle.ellipsis.fleft\").text\n",
    "        location=job.find_element_by_css_selector(\".fleft.grey-text.br2.placeHolderLi.location\").text\n",
    "        exp=job.find_element_by_css_selector(\".fleft.grey-text.br2.placeHolderLi.experience\").text\n",
    "        skills=job.find_element_by_css_selector(\".tags.has-description\").text\n",
    "        jobs[\"roles\"].append(role)\n",
    "        jobs[\"companies\"].append(company)\n",
    "        jobs[\"locations\"].append(location)\n",
    "        jobs[\"experience\"].append(exp)\n",
    "        jobs[\"skills\"].append(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print scrape data\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862629ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform data into dataframe\n",
    "df=pd.DataFrame.from_dict(jobs)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case all the strings to avoid redundancy\n",
    "df=df.apply(lambda x: x.astype(str).str.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are more than one loaction and skills are attached to each job, let split the location and skill\n",
    "df.skills=[skill.split(\"\\n\") for skill in df.skills]\n",
    "df.locations=[location.split(\",\") for location in df.locations]\n",
    "df[15:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72236ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about columns in dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any data have null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing value\n",
    "df=df.dropna()\n",
    "df.to_csv('naukri.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
